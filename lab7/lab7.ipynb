{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14e060bd-5064-4af2-a673-6e149b0b9ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"DataPreprocessing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce14c4d2-459a-489a-be3f-f6a21391d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = spark.read.csv(\"kddcup.data_10_percent_corrected\", header=False, inferSchema=True)\n",
    "\n",
    "# Define the schema based on the dataset\n",
    "# Assuming the columns are unnamed, you'll need to provide column names\n",
    "data = data.toDF(\"col1\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"col7\", \"col8\", \"col9\", \"col10\", \n",
    "                  \"col11\", \"col12\", \"col13\", \"col14\", \"col15\", \"col16\", \"col17\", \"col18\", \"col19\", \"col20\", \n",
    "                  \"col21\", \"col22\", \"col23\", \"col24\", \"col25\", \"col26\", \"col27\", \"col28\", \"col29\", \"col30\", \n",
    "                  \"col31\", \"col32\", \"col33\", \"col34\", \"col35\", \"col36\", \"col37\",\"col38\",\"col39\",\"col40\", \"col41\",\"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "265b6560-e69f-42be-95c0-a41ce552f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# Imputer is used to fill missing values with the mean of each column\n",
    "imputer = Imputer(inputCols=[\"col5\", \"col6\", \"col7\", \"col8\", \"col9\", \"col10\", \"col11\", \"col12\", \"col13\",\n",
    "                             \"col14\", \"col15\", \"col16\", \"col17\", \"col18\", \"col19\", \"col20\", \"col21\", \"col22\",\n",
    "                             \"col23\", \"col24\", \"col25\", \"col26\", \"col27\", \"col28\", \"col29\", \"col30\", \"col31\",\n",
    "                             \"col32\", \"col33\", \"col34\", \"col35\", \"col36\", \"col37\",\"col38\",\"col39\",\"col40\", \"col41\"],\n",
    "                   outputCols=[\"col5\", \"col6\", \"col7\", \"col8\", \"col9\", \"col10\", \"col11\", \"col12\", \"col13\",\n",
    "                               \"col14\", \"col15\", \"col16\", \"col17\", \"col18\", \"col19\", \"col20\", \"col21\", \"col22\",\n",
    "                               \"col23\", \"col24\", \"col25\", \"col26\", \"col27\", \"col28\", \"col29\", \"col30\", \"col31\",\n",
    "                               \"col32\", \"col33\", \"col34\", \"col35\", \"col36\", \"col37\",\"col38\",\"col39\",\"col40\", \"col41\"]).setStrategy(\"mean\")\n",
    "\n",
    "data = imputer.fit(data).transform(data)\n",
    "\n",
    "# Scale numerical features\n",
    "feature_cols = [\"col5\", \"col6\", \"col7\", \"col8\", \"col9\", \"col10\", \"col11\", \"col12\", \"col13\", \n",
    "                \"col14\", \"col15\", \"col16\", \"col17\", \"col18\", \"col19\", \"col20\", \"col21\", \"col22\", \n",
    "                \"col23\", \"col24\", \"col25\", \"col26\", \"col27\", \"col28\", \"col29\", \"col30\", \"col31\", \n",
    "                \"col32\", \"col33\", \"col34\", \"col35\", \"col36\", \"col37\",\"col38\",\"col39\",\"col40\", \"col41\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e51bf76e-58a5-46e5-b909-6d5e0ff9c25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+--------------------+--------------------+\n",
      "|col1|col2|col3|col4|col5|col6|col7|col8|col9|col10|col11|col12|col13|col14|col15|col16|col17|col18|col19|col20|col21|col22|col23|col24|col25|col26|col27|col28|col29|col30|col31|col32|col33|col34|col35|col36|col37|col38|col39|col40|col41|  label|            features|     scaled_features|\n",
      "+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+--------------------+--------------------+\n",
      "|   0| tcp|http|  SF| 181|5450|   0|   0|   0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    8|    8|  0.0|  0.0|  0.0|  0.0|  1.0|  0.0|  0.0|    9|    9|  1.0|  0.0| 0.11|  0.0|  0.0|  0.0|  0.0|  0.0|normal.|(37,[0,1,7,18,19,...|[-0.0028785247840...|\n",
      "|   0| tcp|http|  SF| 239| 486|   0|   0|   0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    8|    8|  0.0|  0.0|  0.0|  0.0|  1.0|  0.0|  0.0|   19|   19|  1.0|  0.0| 0.05|  0.0|  0.0|  0.0|  0.0|  0.0|normal.|(37,[0,1,7,18,19,...|[-0.0028198332867...|\n",
      "|   0| tcp|http|  SF| 235|1337|   0|   0|   0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    8|    8|  0.0|  0.0|  0.0|  0.0|  1.0|  0.0|  0.0|   29|   29|  1.0|  0.0| 0.03|  0.0|  0.0|  0.0|  0.0|  0.0|normal.|(37,[0,1,7,18,19,...|[-0.0028238809762...|\n",
      "+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "data = scaler.fit(data).transform(data)\n",
    "\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4aaf6881-9aa2-41be-865c-9968a394da35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+--------------------+--------------------+-------+\n",
      "|col1|col2|col3|col4|col5|col6|col7|col8|col9|col10|col11|col12|col13|col14|col15|col16|col17|col18|col19|col20|col21|col22|col23|col24|col25|col26|col27|col28|col29|col30|col31|col32|col33|col34|col35|col36|col37|col38|col39|col40|col41|  label|            features|     scaled_features|cluster|\n",
      "+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+--------------------+--------------------+-------+\n",
      "|   0| tcp|http|  SF| 181|5450|   0|   0|   0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    8|    8|  0.0|  0.0|  0.0|  0.0|  1.0|  0.0|  0.0|    9|    9|  1.0|  0.0| 0.11|  0.0|  0.0|  0.0|  0.0|  0.0|normal.|(37,[0,1,7,18,19,...|[-0.0028785247840...|     67|\n",
      "|   0| tcp|http|  SF| 239| 486|   0|   0|   0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    8|    8|  0.0|  0.0|  0.0|  0.0|  1.0|  0.0|  0.0|   19|   19|  1.0|  0.0| 0.05|  0.0|  0.0|  0.0|  0.0|  0.0|normal.|(37,[0,1,7,18,19,...|[-0.0028198332867...|     67|\n",
      "|   0| tcp|http|  SF| 235|1337|   0|   0|   0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    8|    8|  0.0|  0.0|  0.0|  0.0|  1.0|  0.0|  0.0|   29|   29|  1.0|  0.0| 0.03|  0.0|  0.0|  0.0|  0.0|  0.0|normal.|(37,[0,1,7,18,19,...|[-0.0028238809762...|     67|\n",
      "+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+--------------------+--------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# K-means clustering\n",
    "kmeans = KMeans(k=80, seed=1, featuresCol=\"scaled_features\", predictionCol=\"cluster\")\n",
    "model = kmeans.fit(data)\n",
    "clusters = model.transform(data)\n",
    "\n",
    "clusters.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0de7a89a-b979-42db-9728-1fc0f86209e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+--------------------+--------------------+-------+------------------+----------+\n",
      "|col1|col2|col3|col4|col5|col6|col7|col8|col9|col10|col11|col12|col13|col14|col15|col16|col17|col18|col19|col20|col21|col22|col23|col24|col25|col26|col27|col28|col29|col30|col31|col32|col33|col34|col35|col36|col37|col38|col39|col40|col41|  label|            features|     scaled_features|cluster|distance_to_center|is_anomaly|\n",
      "+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+--------------------+--------------------+-------+------------------+----------+\n",
      "|   0| tcp|http|  SF| 181|5450|   0|   0|   0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    8|    8|  0.0|  0.0|  0.0|  0.0|  1.0|  0.0|  0.0|    9|    9|  1.0|  0.0| 0.11|  0.0|  0.0|  0.0|  0.0|  0.0|normal.|(37,[0,1,7,18,19,...|[-0.0028785247840...|     67|         1.5105318|      true|\n",
      "|   0| tcp|http|  SF| 239| 486|   0|   0|   0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    8|    8|  0.0|  0.0|  0.0|  0.0|  1.0|  0.0|  0.0|   19|   19|  1.0|  0.0| 0.05|  0.0|  0.0|  0.0|  0.0|  0.0|normal.|(37,[0,1,7,18,19,...|[-0.0028198332867...|     67|         1.4017612|      true|\n",
      "|   0| tcp|http|  SF| 235|1337|   0|   0|   0|    0|    0|    1|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    8|    8|  0.0|  0.0|  0.0|  0.0|  1.0|  0.0|  0.0|   29|   29|  1.0|  0.0| 0.03|  0.0|  0.0|  0.0|  0.0|  0.0|normal.|(37,[0,1,7,18,19,...|[-0.0028238809762...|     67|         1.3096797|      true|\n",
      "+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------+--------------------+--------------------+-------+------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "import numpy as np\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Get cluster centers\n",
    "centers = np.array(model.clusterCenters())\n",
    "\n",
    "# Calculate distances from points to cluster centers\n",
    "def distance_to_center(features, center):\n",
    "    return float(np.sqrt(np.sum((np.array(features) - np.array(center)) ** 2)))\n",
    "\n",
    "distance_udf = udf(lambda features: min([distance_to_center(features, center) for center in centers]), FloatType())\n",
    "\n",
    "clusters = clusters.withColumn(\"distance_to_center\", distance_udf(col(\"scaled_features\")))\n",
    "\n",
    "# Define anomaly if distance is greater than a threshold\n",
    "threshold = 1.0  # Set a suitable threshold\n",
    "clusters = clusters.withColumn(\"is_anomaly\", col(\"distance_to_center\") > threshold)\n",
    "\n",
    "clusters.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "55f000a8-9ab3-43b0-a215-e9dd78dbec1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 224:>                                                      (0 + 18) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared Euclidean distance = 0.8907618402151812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = ClusteringEvaluator(featuresCol=\"scaled_features\", predictionCol=\"cluster\")\n",
    "silhouette = evaluator.evaluate(clusters)\n",
    "print(f\"Silhouette with squared Euclidean distance = {silhouette}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47fa3fa6-0ff7-49c5-9398-9a80eb4eaf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 239:================================================>      (16 + 2) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of anomalies: 41775\n",
      "Number of normal points: 452246\n",
      "Total number of points: 494021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the number of anomalies and normal points\n",
    "num_anomalies = clusters.filter(col(\"is_anomaly\")).count()\n",
    "num_normal = clusters.filter(~col(\"is_anomaly\")).count()\n",
    "total_count = clusters.count()\n",
    "\n",
    "print(f\"Number of anomalies: {num_anomalies}\")\n",
    "print(f\"Number of normal points: {num_normal}\")\n",
    "print(f\"Total number of points: {total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d2ca7-1cb4-47c5-bdbf-6ddf8703e124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f569ec64-a038-44f0-b188-12a13f5cce31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
