{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, trim, regexp_replace, udf, explode, collect_set, lit, count, when\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "import pyspark.sql.functions as F\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EntityResolutionPreprocessing\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-----------+------------------+--------------+---------------+\n",
      "|id |name_normalized|name_tokens|address_normalized|address_tokens|city_normalized|\n",
      "+---+---------------+-----------+------------------+--------------+---------------+\n",
      "|1  |company abc inc|[abc]      |1234 elm st       |[1234, elm]   |new york       |\n",
      "|2  |abc company inc|[abc]      |1234 elm street   |[1234, elm]   |new york       |\n",
      "|3  |xyz corp       |[xyz]      |5678 oak ave      |[5678, oak]   |los angeles    |\n",
      "+---+---------------+-----------+------------------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"1\", \"Company ABC, Inc.\", \"1234 Elm St.\", \"New York\", \"NY\"),\n",
    "    (\"2\", \"ABC Company, Inc.\", \"1234 Elm Street\", \"New York\", \"NY\"),\n",
    "    (\"3\", \"XYZ Corp.\", \"5678 Oak Ave.\", \"Los Angeles\", \"CA\")\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"address\", \"city\", \"state\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Step 1: Normalize text (convert to lowercase, remove punctuation, trim whitespace)\n",
    "def normalize_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.strip()  # Trim leading/trailing whitespace\n",
    "    return text\n",
    "\n",
    "normalize_text_udf = udf(lambda x: normalize_text(x), StringType())\n",
    "\n",
    "df = df.withColumn(\"name_normalized\", normalize_text_udf(col(\"name\")))\n",
    "df = df.withColumn(\"address_normalized\", normalize_text_udf(col(\"address\")))\n",
    "df = df.withColumn(\"city_normalized\", normalize_text_udf(col(\"city\")))\n",
    "\n",
    "# Step 2: Tokenize text (split into words)\n",
    "tokenizer = Tokenizer(inputCol=\"name_normalized\", outputCol=\"name_tokens\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"address_normalized\", outputCol=\"address_tokens\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "# Step 3: Remove stopwords (optional)\n",
    "# Assuming a list of stopwords\n",
    "stopwords = ['inc', 'co', 'corp', 'llc', 'company', 'street', 'st', 'ave', 'avenue']\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "remove_stopwords_udf = udf(lambda x: remove_stopwords(x), ArrayType(StringType()))\n",
    "\n",
    "df = df.withColumn(\"name_tokens\", remove_stopwords_udf(col(\"name_tokens\")))\n",
    "df = df.withColumn(\"address_tokens\", remove_stopwords_udf(col(\"address_tokens\")))\n",
    "\n",
    "# Step 4: Remove duplicates and null values\n",
    "df = df.dropDuplicates()\n",
    "df = df.na.drop()\n",
    "\n",
    "# Step 5: View the preprocessed DataFrame\n",
    "df.select(\"id\", \"name_normalized\", \"name_tokens\", \"address_normalized\", \"address_tokens\", \"city_normalized\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+\n",
      "|id1|id2|similarity|\n",
      "+---+---+----------+\n",
      "|1  |2  |1.0       |\n",
      "|1  |3  |0.0       |\n",
      "|1  |4  |0.25      |\n",
      "|2  |1  |1.0       |\n",
      "|2  |3  |0.0       |\n",
      "|2  |4  |0.25      |\n",
      "|3  |1  |0.0       |\n",
      "|3  |2  |0.0       |\n",
      "|3  |4  |0.33333334|\n",
      "|4  |1  |0.25      |\n",
      "|4  |2  |0.25      |\n",
      "|4  |3  |0.33333334|\n",
      "+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import explode, collect_set\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimilarityScoreComputation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"1\", \"Company ABC Inc.\"),\n",
    "    (\"2\", \"ABC Company Inc.\"),\n",
    "    (\"3\", \"XYZ Corp.\"),\n",
    "    (\"4\", \"Company XYZ\")\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Tokenize text data\n",
    "tokenizer = Tokenizer(inputCol=\"name\", outputCol=\"name_tokens\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "# Create a DataFrame with (id, tokens) for self-joins\n",
    "df_tokens = df.select(\"id\", \"name_tokens\")\n",
    "\n",
    "# Cross join the DataFrame with itself to compute pairwise similarity\n",
    "df_cross = df_tokens.alias(\"df1\").crossJoin(df_tokens.alias(\"df2\"))\n",
    "\n",
    "# Define UDF to compute Jaccard Similarity\n",
    "def jaccard_similarity(set1, set2):\n",
    "    set1 = set(set1)\n",
    "    set2 = set(set2)\n",
    "    if len(set1.union(set2)) == 0:\n",
    "        return 0.0\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "jaccard_similarity_udf = udf(jaccard_similarity, FloatType())\n",
    "\n",
    "# Compute Jaccard Similarity for each pair\n",
    "df_similarity = df_cross.withColumn(\n",
    "    \"similarity\",\n",
    "    jaccard_similarity_udf(col(\"df1.name_tokens\"), col(\"df2.name_tokens\"))\n",
    ")\n",
    "# Filter out self-comparisons\n",
    "df_similarity = df_similarity.filter(col(\"df1.id\") != col(\"df2.id\"))\n",
    "\n",
    "# Show results\n",
    "df_similarity.select(\n",
    "    col(\"df1.id\").alias(\"id1\"),\n",
    "    col(\"df2.id\").alias(\"id2\"),\n",
    "    col(\"similarity\")\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0000\n",
      "Recall: 0.5000\n",
      "F1-Score: 0.6667\n"
     ]
    }
   ],
   "source": [
    "# Sample ground truth data (true matches and non-matches)\n",
    "# Format: (id1, id2, is_match)\n",
    "ground_truth_data = [\n",
    "    (\"1\", \"2\", 1),\n",
    "    (\"2\", \"3\", 0),\n",
    "    (\"3\", \"4\", 1),\n",
    "    (\"4\", \"1\", 0)\n",
    "]\n",
    "\n",
    "columns = [\"id1\", \"id2\", \"is_match\"]\n",
    "\n",
    "# Create DataFrame for ground truth\n",
    "df_ground_truth = spark.createDataFrame(ground_truth_data, columns)\n",
    "\n",
    "# Sample predictions from the entity resolution model\n",
    "# Format: (id1, id2, predicted_match)\n",
    "predictions_data = [\n",
    "    (\"1\", \"2\", 1),\n",
    "    (\"2\", \"3\", 0),\n",
    "    (\"3\", \"4\", 0),\n",
    "    (\"4\", \"1\", 0)\n",
    "]\n",
    "\n",
    "columns_pred = [\"id1\", \"id2\", \"predicted_match\"]\n",
    "\n",
    "# Create DataFrame for predictions\n",
    "df_predictions = spark.createDataFrame(predictions_data, columns_pred)\n",
    "\n",
    "# Join ground truth and predictions on id1 and id2\n",
    "df_joined = df_ground_truth.alias(\"gt\").join(\n",
    "    df_predictions.alias(\"pred\"),\n",
    "    (col(\"gt.id1\") == col(\"pred.id1\")) & (col(\"gt.id2\") == col(\"pred.id2\")),\n",
    "    \"left\"\n",
    ").select(\n",
    "    col(\"gt.id1\"),\n",
    "    col(\"gt.id2\"),\n",
    "    col(\"gt.is_match\"),\n",
    "    col(\"pred.predicted_match\")\n",
    ")\n",
    "\n",
    "# Compute True Positives, False Positives, True Negatives, and False Negatives\n",
    "df_metrics = df_joined.withColumn(\n",
    "    \"true_positive\",\n",
    "    when((col(\"is_match\") == 1) & (col(\"predicted_match\") == 1), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"false_positive\",\n",
    "    when((col(\"is_match\") == 0) & (col(\"predicted_match\") == 1), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"true_negative\",\n",
    "    when((col(\"is_match\") == 0) & (col(\"predicted_match\") == 0), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"false_negative\",\n",
    "    when((col(\"is_match\") == 1) & (col(\"predicted_match\") == 0), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Aggregate counts\n",
    "metrics = df_metrics.agg(\n",
    "    count(when(col(\"true_positive\") == 1, 1)).alias(\"true_positives\"),\n",
    "    count(when(col(\"false_positive\") == 1, 1)).alias(\"false_positives\"),\n",
    "    count(when(col(\"true_negative\") == 1, 1)).alias(\"true_negatives\"),\n",
    "    count(when(col(\"false_negative\") == 1, 1)).alias(\"false_negatives\")\n",
    ").collect()[0]\n",
    "\n",
    "# Extract counts\n",
    "true_positives = metrics.true_positives\n",
    "false_positives = metrics.false_positives\n",
    "true_negatives = metrics.true_negatives\n",
    "false_negatives = metrics.false_negatives\n",
    "\n",
    "# Compute Precision, Recall, and F1-Score\n",
    "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
    "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "# Print results\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
